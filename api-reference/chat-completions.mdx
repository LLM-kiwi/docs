---
title: "Chat Completions"
description: "Generate intelligent text responses using state-of-the-art AI models"
---

The Chat Completions API is your primary interface for building chatbots, virtual assistants, and text generation applications.

## Endpoint

```bash
POST https://api.llm.kiwi/v1/chat/completions
```

## Request Parameters

<ParamField body="model" type="string" required>
  Model ID: `default` (Free), or any Pro model (e.g. `gpt-oss-20b`).
</ParamField>

<ParamField body="messages" type="array" required>
  Conversation history as message objects.
</ParamField>

<ParamField body="stream" type="boolean" default="false">
  Enable streaming responses.
</ParamField>

<ParamField body="temperature" type="number" default="1">
  Sampling temperature (0-2).
</ParamField>

<ParamField body="max_tokens" type="integer">
  Maximum tokens to generate.
</ParamField>

<ParamField body="response_format" type="object">
  Set `{ "type": "json_object" }` for JSON mode. <sup>Pro</sup>
</ParamField>

<ParamField body="tools" type="array">
  Function definitions for tool calling. <sup>Pro</sup>
</ParamField>

<ParamField body="tool_choice" type="string/object">
  Control tool selection behavior. <sup>Pro</sup>
</ParamField>

## Message Format

Each message has a `role` and `content`:

| Role | Description |
| :--- | :--- |
| `system` | Sets the assistant's behavior and persona. |
| `user` | Input from the end user. |
| `assistant` | Previous model responses for context. |
| `tool` | Results from function/tool calls. |

### Example Conversation

```json
[
  {"role": "system", "content": "You are a helpful coding assistant."},
  {"role": "user", "content": "How do I reverse a string in Python?"}
]
```

## Basic Request

<CodeGroup>
```python Python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.llm.kiwi/v1",
    api_key="YOUR_API_KEY"
)

response = client.chat.completions.create(
    model="default",
    messages=[{"role": "user", "content": "Hello!"}]
)

print(response.choices[0].message.content)
```

```javascript Node.js
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://api.llm.kiwi/v1',
  apiKey: process.env.LLM_KIWI_API_KEY
});

const response = await client.chat.completions.create({
  model: 'default',
  messages: [{ role: 'user', content: 'Hello!' }]
});

console.log(response.choices[0].message.content);
```
</CodeGroup>

## Streaming Responses

Enable streaming for real-time token delivery:

```python Python
stream = client.chat.completions.create(
    model="default",
    messages=[{"role": "user", "content": "Tell me a story."}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## Response Object

```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1706745600,
  "model": "gpt-oss-20b",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Use slicing: `my_string[::-1]`"
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 12,
    "completion_tokens": 20,
    "total_tokens": 32
  }
}
```

---

## Pro Features

<CardGroup cols={2}>
  <Card title="JSON Mode" icon="brackets-curly" href="/api-reference/json-mode">
    Get structured JSON responses with guaranteed valid output.
  </Card>
  <Card title="Function Calling" icon="function" href="/api-reference/function-calling">
    Enable models to call your functions and APIs.
  </Card>
</CardGroup>

